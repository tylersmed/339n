i = 1
n = 1
directions = unlist(strsplit(directions, ""))
for (direction in directions) {
if (direction == "X") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
i = i + 1
n = n + 1
} else if (direction == "L") {
seq1_aligned = paste(seq1_aligned, "_", sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
n = n + 1
} else if (direction == "D") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, "_", sep="")
i = i + 1
}
}
return(list(seq1_aligned, seq2_aligned))
}
global_align("UUUA","CUA")
global_align("UUUA","CUA", gap_open = -5)
global_align("UUUA","CUAUUA", gap_open = -5)
global_align("UUUA","CUAUUA")
global_align("UUUA","CUAUUA", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-8)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("GATTACA","GCATGCT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("GATTACA","GCATGCAT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AATCAGGT","CGTTTAGAC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align = function(seq1, seq2, match_score = 3, mismatch_score = -10, gap_open = -3) {
m = nchar(seq1)
n = nchar(seq2)
score_matrix = matrix(0, nrow = m+1, ncol = n+1)
# fill out the matrix
for (i in 1:(m+1)) {
for (j in 1:(n+1)) {
if (i == 1) {
score_matrix[i,j] = gap_open * (j-1)
} else if (j == 1) {
score_matrix[i, j] = gap_open * (i-1)
} else {
match = score_matrix[i-1, j-1] + ifelse(substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1), match_score, mismatch_score)
gap_right = score_matrix[i, j-1] + gap_open
gap_down = score_matrix[i-1, j] + gap_open
score_matrix[i, j] = max(match, gap_right, gap_down)
}
}
}
# trace the matrix back
directions = traceback_matrix(score_matrix, m+1, n+1, seq1, seq2)
directions = gsub(" ", "", directions)
translated = translate_directions(directions, seq1, seq2)
print(translated[[1]])
print(translated[[2]])
print(paste("Score:", score_matrix[m+1, n+1]))
}
# Recursively trace back the matrix and return a string of the directions to take
traceback_matrix = function(score_matrix, i, j, seq1, seq2, match, mismatch, gap) {
if (i==1 && j==1) {
return("")
}
current = score_matrix[i, j]
if (i == 1) {
# Only possible to come from the left
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), " L"))
} else if (j == 1) {
# Only possible to come from above
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), " D"))
}
# Possible directions: diagonal, left, up
diag = (substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] + 3) ||
(substr(seq1, i-1, i-1) != substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] - 10)
left = score_matrix[i, j-1]-3 == current
up = score_matrix[i-1, j]-3 == current
if (diag) {
return(paste(traceback_matrix(score_matrix, i-1, j-1, seq1, seq2, match, mismatch, gap), "X"))
} else if (left) {
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), "L"))
} else {
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), "D"))
}
}
#translate directions to alignment
translate_directions = function(directions, seq1, seq2) {
seq1_aligned = ""
seq2_aligned = ""
i = 1
n = 1
directions = unlist(strsplit(directions, ""))
for (direction in directions) {
if (direction == "X") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
i = i + 1
n = n + 1
} else if (direction == "L") {
seq1_aligned = paste(seq1_aligned, "_", sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
n = n + 1
} else if (direction == "D") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, "_", sep="")
i = i + 1
}
}
return(list(seq1_aligned, seq2_aligned))
}
global_align("AATCAGGT","CGTTTAGAC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-6)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-5)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-4)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-3)
setwd("~/git/339n/class-notes")
renv::autoload()
renv::status()
library (dslabs)
str(brca)
?brca
brca$y
set.seed(3)
install.packages("caret")
#install.packages("caret")
library (caret)
library (ggplot2)
brca_df = data.frame(Target=brca$y, Feature= brca$x)
## In the previous class we used the sample function
?sample
inTrain <- sample (1:nrow(brca_df), replace=F, size= round(nrow(brca_df)*0.9))
training = brca_df[inTrain, ]
testing = brca_df[-inTrain,]
str(training)
brca_df = data.frame(Target=brca$y, Feature= brca$x)
brca_df = data.frame(Target=brca$y, Feature= brca$x)
## In the previous class we used the sample function
?sample
library (dslabs)
str(brca)
?brca
brca$y
set.seed(3)
#install.packages("caret")
library (caret)
library (ggplot2)
brca_df = data.frame(Target=brca$y, Feature= brca$x)
## In the previous class we used the sample function
?sample
inTrain <- sample (1:nrow(brca_df), replace=F, size= round(nrow(brca_df)*0.9))
training = brca_df[inTrain, ]
testing = brca_df[-inTrain,]
str(training)
str(testing)
## Let's use createDataPartition() function from the caret package
## for a more robust sampling strategy.
## to maintain the ratio of target classes in both training and testing sets.
## Split data into training and testing sets
set.seed(3)
inTrain <- createDataPartition(y = brca_df$Target, p = 0.9, list = FALSE)
training <- brca_df[inTrain, ]
testing <- brca_df[-inTrain, ]
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
install.packages('cowplot')
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
#install.packages('cowplot')
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
?theme_cowplot
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
??theme_cowplot
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
cowplot::theme_cowplot()
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + cowplot::theme_cowplot(12)
p2 <-ggplot (training, aes(x=Feature.perimeter_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1)+cowplot::theme_cowplot(12)
p3 <-ggplot (training, aes(x=Feature.symmetry_worst, fill= Target, color= Target)) + geom_boxplot() + cowplot::theme_cowplot(12)
p4 <-ggplot (training, aes(x=Feature.symmetry_se, fill= Target, color= Target)) + geom_boxplot() + cowplot::theme_cowplot(12)
# Combine the plots into a grid
plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
??plot_grid
# Combine the plots into a grid
cowplot::plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
#install.packages("tree")
library(tree)
install.packages("tree")
#install.packages("tree")
library(tree)
#single split
decision_tree1 = tree(Target~Feature.radius_mean, data=training, control=tree.control(nrow(training), mincut=200))
summary(decision_tree1)
plot(decision_tree1)
text(decision_tree1, pretty=0)
#multi-feature tree
decision_tree3= tree(Target~ ., data=training )
summary(decision_tree3)
plot(decision_tree3)
text(decision_tree3, pretty=0)
tree.pred = predict(decision_tree3, testing, type="class")
table(tree.pred, testing$Target)
head (brca_df)
brca_df$Target
training_two = brca_df[1:360,]
testing_two = brca_df[360:562,]
dim(training)
dim(training_two)
#train a decision tree
decision_tree4= tree(Target~., data=training_two)
summary(decision_tree4)
summary(decision_tree3)
tree.pred = predict(decision_tree3, testing, type="class")
sum (tree.pred == testing$Target)/nrow(testing)
tree.pred2 = predict(decision_tree4, testing_two, type="class")
sum(tree.pred2 ==testing_two$Target)/nrow(testing_two)
decision_tree5= tree(Target~Feature.radius_mean + Feature.symmetry_mean, data=training,
control = tree.control(nrow(training), minsize = 2, mindev=0))
library (cowplot)
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
p2 <-ggplot (training, aes(x=Feature.perimeter_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1)+ theme_cowplot(12)
p3 <-ggplot (training, aes(x=Feature.symmetry_worst, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
p4 <-ggplot (training, aes(x=Feature.symmetry_se, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
# Combine the plots into a grid
plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
sum(tree.pred3 == testing$Target)/nrow(testing)
head (brca_df)
brca_df$Target
## Let's use createDataPartition() function from the caret package
## for a more robust sampling strategy.
## to maintain the ratio of target classes in both training and testing sets.
## Split data into training and testing sets
set.seed(3)
inTrain <- createDataPartition(y = brca_df$Target, p = 0.9, list = FALSE)
training <- brca_df[inTrain, ]
testing <- brca_df[-inTrain, ]
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
cowplot::theme_cowplot()
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
p2 <-ggplot (training, aes(x=Feature.perimeter_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1)+ theme_cowplot(12)
p3 <-ggplot (training, aes(x=Feature.symmetry_worst, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
p4 <-ggplot (training, aes(x=Feature.symmetry_se, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
# Combine the plots into a grid
plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
#install.packages("tree")
library(tree)
#single split
decision_tree1 = tree(Target~Feature.radius_mean, data=training, control=tree.control(nrow(training), mincut=200))
summary(decision_tree1)
plot(decision_tree1)
text(decision_tree1, pretty=0)
#multi-feature tree
decision_tree3= tree(Target~ ., data=training )
summary(decision_tree3)
plot(decision_tree3)
text(decision_tree3, pretty=0)
tree.pred = predict(decision_tree3, testing, type="class")
table(tree.pred, testing$Target)
head (brca_df)
brca_df$Target
training_two = brca_df[1:360,]
testing_two = brca_df[360:562,]
dim(training)
dim(training_two)
#train a decision tree
decision_tree4= tree(Target~., data=training_two)
summary(decision_tree4)
summary(decision_tree3)
library (dslabs)
str(brca)
?brca
#target variable brca$y
brca$y
set.seed(3)
#install.packages("caret")
library (caret)
library (ggplot2)
library (cowplot)
brca_df = data.frame(Target=brca$y, Feature= brca$x)
## In the previous class we used the sample function
?sample
inTrain <- sample (1:nrow(brca_df), replace=F, size= round(nrow(brca_df)*0.9))
training = brca_df[inTrain, ]
testing = brca_df[-inTrain,]
str(training)
str(testing)
## Let's use createDataPartition() function from the caret package
## for a more robust sampling strategy.
## to maintain the ratio of target classes in both training and testing sets.
## Split data into training and testing sets
set.seed(3)
inTrain <- createDataPartition(y = brca_df$Target, p = 0.9, list = FALSE)
training <- brca_df[inTrain, ]
testing <- brca_df[-inTrain, ]
#Question-1: createDataPartition()
#https://www.investopedia.com/terms/stratified_random_sampling.asp#:~:text=Stratified%20random%20sampling%20is%20a,as%20income%20or%20educational%20attainment.
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
cowplot::theme_cowplot()
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
p2 <-ggplot (training, aes(x=Feature.perimeter_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1)+ theme_cowplot(12)
p3 <-ggplot (training, aes(x=Feature.symmetry_worst, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
p4 <-ggplot (training, aes(x=Feature.symmetry_se, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
# Combine the plots into a grid
plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
#install.packages("tree")
library(tree)
#single split
decision_tree1 = tree(Target~Feature.radius_mean, data=training, control=tree.control(nrow(training), mincut=200))
summary(decision_tree1)
plot(decision_tree1)
text(decision_tree1, pretty=0)
#multi-feature tree
decision_tree3= tree(Target~ ., data=training )
summary(decision_tree3)
plot(decision_tree3)
text(decision_tree3, pretty=0)
tree.pred = predict(decision_tree3, testing, type="class")
table(tree.pred, testing$Target)
head (brca_df)
brca_df$Target
training_two = brca_df[1:360,]
testing_two = brca_df[360:562,]
dim(training)
dim(training_two)
#train a decision tree
decision_tree4= tree(Target~., data=training_two)
summary(decision_tree4)
summary(decision_tree3)
tree.pred = predict(decision_tree3, testing, type="class")
sum (tree.pred == testing$Target)/nrow(testing)
tree.pred2 = predict(decision_tree4, testing_two, type="class")
sum(tree.pred2 ==testing_two$Target)/nrow(testing_two)
#What happened? Q2
decision_tree5= tree(Target~Feature.radius_mean + Feature.symmetry_mean, data=training,
control = tree.control(nrow(training), minsize = 2, mindev=0))
## minsize = 2: the smallest size of any terminal node (leaf) should be 2.
## the algorithm will not attempt to split a node if it contains fewer than 2 cases.
#mindev = 0 sets the minimum decrease in the deviance required to attempt any split.
## Any decrease is sufficient to make a split:
## it does not require a specific minimum amount of improvement in model fit
## to justify a split.
decision_tree6 = tree(Target ~Feature.radius_mean + Feature.symmetry_mean, data=training)
summary(decision_tree5)
summary(decision_tree6)
plot (decision_tree5)
plot (decision_tree6)
tree.pred3 = predict(decision_tree5, testing, type="class")
tree.pred4 = predict(decision_tree6, testing, type="class")
sum(tree.pred3 == testing$Target)/nrow(testing)
sum(tree.pred4 == testing$Target)/nrow(testing)
# the simpler tree (decision_tree6) gave better predictions on the testing set.
#Why? Q3
predict(decision_tree6, testing, type="class")
predict(decision_tree6, testing, type="vector")
# result as a category vs probabilistic
table(tree.pred4, testing$Target)
#false positive rate FPR -- FP/(FP+TN)
# true positive Rate /Sensitivity-- TP/FP+FN
# true negative rate /Specificity -- TN/TN+FP
#precision -positive predictive value -- TP/TP + FP
install.packages("randomForest")
rf1 = randomForest(Target ~ ., data= training, importance =T)
y
install.packages("randomForest")
# result as a category vs probabilistic
table(tree.pred4, testing$Target)
#install.packages("randomForest")
library(randomForest)
rf1 = randomForest(Target ~ ., data= training, importance =T)
rf1
rf_predict = predict(rf1, testing)
sum (rf_predict == testing$Target) /nrow(testing)
importance(rf1)
#visualize importance
varImpPlot(rf1)
## Using caret to tune the model
## tuning parameters such as the number of trees, the depth of trees,
## and the number of features considered at each split.
## using the train function from the caret package with 10-fold cross-validation
## What is cross-validation and why it might not be as relevant for RandomForest algorithm?
## Q5 : Crossvalidation
control <- trainControl(method="cv", number=10)
tuneGrid <- expand.grid(.mtry=c(1:sqrt(ncol(training)-1)))
rfTuned <- train(Target~., data=training, method="rf", trControl=control, tuneGrid=tuneGrid)
rfTuned
rfTuned_predict = predict(rfTuned, testing)
sum (rfTuned_predict == testing$Target) /nrow(testing)
## Partial Dependence Plots show the dependence
## between the target function and a set of ‘interesting’ features
# Using the pdp package for partial dependence plots
install.packages("pdp")
## Partial Dependence Plots show the dependence
## between the target function and a set of ‘interesting’ features
# Using the pdp package for partial dependence plots
#install.packages("pdp")
library(pdp)
partial1 <- partial(rfTuned, pred.var = "Feature.perimeter_worst", train = training)
autoplot(partial1)
dev.off()
#"y-hat," represents the predicted or estimated value of the target variable
partial2 <- partial(rfTuned, pred.var = "Feature.radius_mean", train = training)
autoplot(partial2)
# Using randomForest for clustering
rfClust <- randomForest(x=training[-ncol(training)], y=NULL, proximity=TRUE)
# Since y=NULL, there is no target variable,function performs unsupervised learning.
#  proximity=TRUE computes a proximity matrix, which measures the similarity
# between each pair of samples in the data
# two samples are more similar if they end up in the same terminal nodes of trees.
str(rfClust)
#calculate dissimilarity from proximity scores
dissimilarity <- 1 - rfClust$proximity
#perform hierarchical clustering on the dissimilarity matrix
hc <- hclust(as.dist(dissimilarity), method="average")
plot(hc)
##Discussion: How do we choose between using Naive Bayes classification and Random Forest?
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
## because of its ability to handle noisy and complex data.
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
## because of its ability to handle noisy and complex data.
## Naive Bayes is used in genomics for tasks like gene expression classification,
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
## because of its ability to handle noisy and complex data.
## Naive Bayes is used in genomics for tasks like gene expression classification,
## patient diagnosis based on genetic variation, or phylogenetics classification etc..
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
## because of its ability to handle noisy and complex data.
## Naive Bayes is used in genomics for tasks like gene expression classification,
## patient diagnosis based on genetic variation, or phylogenetics classification etc..
## Difference in efficiency, interpretability, data size, class imbalance
##Random Forest is often used in bioinformatics for tasks like predicting protein-protein
## interactions, gene selection for disease classification, or biomarker discovery
## because of its ability to handle noisy and complex data.
## Naive Bayes is used in genomics for tasks like gene expression classification,
## patient diagnosis based on genetic variation, or phylogenetics classification etc..
## Difference in efficiency, interpretability, data size, class imbalance
#Example: Use Naive Bayes when When dealing with high-dimensional data where features are independent and computational efficiency is critical.
summary(decision_tree1)
plot(decision_tree1)
text(decision_tree1, pretty=0)
#multi-feature tree
decision_tree3= tree(Target~ ., data=training )
summary(decision_tree3)
plot(decision_tree3)
text(decision_tree3, pretty=0)
text(decision_tree1, pretty=0)
plot(decision_tree1)
text(decision_tree1, pretty=0)
#multi-feature tree
decision_tree3= tree(Target~ ., data=training )
summary(decision_tree3)
plot(decision_tree3)
text(decision_tree3, pretty=0)
tree.pred = predict(decision_tree3, testing, type="class")
table(tree.pred, testing$Target)
rf1 = randomForest(Target ~ ., data= training, importance =T)
rf1
rf_predict = predict(rf1, testing)
sum (rf_predict == testing$Target) /nrow(testing)
importance(rf1)
#visualize importance
varImpPlot(rf1)
## Using caret to tune the model
## tuning parameters such as the number of trees, the depth of trees,
## and the number of features considered at each split.
## using the train function from the caret package with 10-fold cross-validation
## What is cross-validation and why it might not be as relevant for RandomForest algorithm?
## Q5 : Crossvalidation
control <- trainControl(method="cv", number=10)
tuneGrid <- expand.grid(.mtry=c(1:sqrt(ncol(training)-1)))
rfTuned <- train(Target~., data=training, method="rf", trControl=control, tuneGrid=tuneGrid)
rfTuned
rfTuned_predict = predict(rfTuned, testing)
sum (rfTuned_predict == testing$Target) /nrow(testing)
## Partial Dependence Plots show the dependence
## between the target function and a set of ‘interesting’ features
# Using the pdp package for partial dependence plots
#install.packages("pdp")
library(pdp)
partial1 <- partial(rfTuned, pred.var = "Feature.perimeter_worst", train = training)
autoplot(partial1)
dev.off()
#"y-hat," represents the predicted or estimated value of the target variable
partial2 <- partial(rfTuned, pred.var = "Feature.radius_mean", train = training)
autoplot(partial2)
# Using randomForest for clustering
rfClust <- randomForest(x=training[-ncol(training)], y=NULL, proximity=TRUE)
# Since y=NULL, there is no target variable,function performs unsupervised learning.
#  proximity=TRUE computes a proximity matrix, which measures the similarity
# between each pair of samples in the data
# two samples are more similar if they end up in the same terminal nodes of trees.
str(rfClust)
#calculate dissimilarity from proximity scores
dissimilarity <- 1 - rfClust$proximity
#perform hierarchical clustering on the dissimilarity matrix
hc <- hclust(as.dist(dissimilarity), method="average")
plot(hc)
