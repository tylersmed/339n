diag = (substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] + 3) ||
(substr(seq1, i-1, i-1) != substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] - 10)
left = score_matrix[i, j-1]-8 == current
up = score_matrix[i-1, j]-8 == current
print(paste(diag, left, up))
#
if (diag) {
return(paste(traceback_matrix(score_matrix, i-1, j-1, seq1, seq2), "X"))
} else if (left) {
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2), "L"))
} else {
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2), "D"))
}
}
#translate directions to alignment
translate_directions = function(directions, seq1, seq2) {
seq1_aligned = ""
seq2_aligned = ""
i = 1
n = 1
directions = unlist(strsplit(directions, ""))
for (direction in directions) {
if (direction == "X") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
i = i + 1
n = n + 1
} else if (direction == "L") {
seq1_aligned = paste(seq1_aligned, "_", sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
n = n + 1
} else if (direction == "D") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, "_", sep="")
i = i + 1
}
}
return(list(seq1_aligned, seq2_aligned))
}
global_align("UUUA","CUA")
# fill out the matrix
for (i in 1:(m+1)) {
for (j in 1:(n+1)) {
if (i == 1) {
score_matrix[i,j] = gap_open * (j-1)
} else if (j == 1) {
score_matrix[i, j] = gap_open * (i-1)
} else {
match = score_matrix[i-1, j-1] + ifelse(substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1), match_score, mismatch_score)
gap_right = score_matrix[i, j-1] + gap_open
gap_down = score_matrix[i-1, j] + gap_open
score_matrix[i, j] = max(match, gap_right, gap_down)
}
}
}
global_align = function(seq1, seq2, match_score = 3, mismatch_score = -10, gap_open = -3) {
m = nchar(seq1)
n = nchar(seq2)
score_matrix = matrix(0, nrow = m+1, ncol = n+1)
# fill out the matrix
for (i in 1:(m+1)) {
for (j in 1:(n+1)) {
if (i == 1) {
score_matrix[i,j] = gap_open * (j-1)
} else if (j == 1) {
score_matrix[i, j] = gap_open * (i-1)
} else {
match = score_matrix[i-1, j-1] + ifelse(substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1), match_score, mismatch_score)
gap_right = score_matrix[i, j-1] + gap_open
gap_down = score_matrix[i-1, j] + gap_open
score_matrix[i, j] = max(match, gap_right, gap_down)
}
}
}
print(score_matrix)
# trace the matrix back
directions = traceback_matrix(score_matrix, m+1, n+1, seq1, seq2)
directions = gsub(" ", "", directions)
print(directions)
translated = translate_directions(directions, seq1, seq2)
print(translated[[1]])
print(translated[[2]])
print(paste("Score:", score_matrix[m+1, n+1]))
}
# Recursively trace back the matrix and return a string of the directions to take
traceback_matrix = function(score_matrix, i, j, seq1, seq2, match, mismatch, gap) {
if (i==1 && j==1) {
return("")
}
current = score_matrix[i, j]
if (i == 1) {
# Only possible to come from the left
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), " L"))
} else if (j == 1) {
# Only possible to come from above
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), " D"))
}
# Possible directions: diagonal, left, up
diag = (substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] + 3) ||
(substr(seq1, i-1, i-1) != substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] - 10)
left = score_matrix[i, j-1]-3 == current
up = score_matrix[i-1, j]-3 == current
print(paste(diag, left, up))
#
if (diag) {
return(paste(traceback_matrix(score_matrix, i-1, j-1, seq1, seq2, match, mismatch, gap), "X"))
} else if (left) {
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), "L"))
} else {
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), "D"))
}
}
#translate directions to alignment
translate_directions = function(directions, seq1, seq2) {
seq1_aligned = ""
seq2_aligned = ""
i = 1
n = 1
directions = unlist(strsplit(directions, ""))
for (direction in directions) {
if (direction == "X") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
i = i + 1
n = n + 1
} else if (direction == "L") {
seq1_aligned = paste(seq1_aligned, "_", sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
n = n + 1
} else if (direction == "D") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, "_", sep="")
i = i + 1
}
}
return(list(seq1_aligned, seq2_aligned))
}
global_align("UUUA","CUA")
global_align("UUUA","CUA", gap_open = -5)
global_align("UUUA","CUAUUA", gap_open = -5)
global_align("UUUA","CUAUUA")
global_align("UUUA","CUAUUA", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-8)
global_align("AGCT","CGT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("GATTACA","GCATGCT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("GATTACA","GCATGCAT", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("AATCAGGT","CGTTTAGAC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align = function(seq1, seq2, match_score = 3, mismatch_score = -10, gap_open = -3) {
m = nchar(seq1)
n = nchar(seq2)
score_matrix = matrix(0, nrow = m+1, ncol = n+1)
# fill out the matrix
for (i in 1:(m+1)) {
for (j in 1:(n+1)) {
if (i == 1) {
score_matrix[i,j] = gap_open * (j-1)
} else if (j == 1) {
score_matrix[i, j] = gap_open * (i-1)
} else {
match = score_matrix[i-1, j-1] + ifelse(substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1), match_score, mismatch_score)
gap_right = score_matrix[i, j-1] + gap_open
gap_down = score_matrix[i-1, j] + gap_open
score_matrix[i, j] = max(match, gap_right, gap_down)
}
}
}
# trace the matrix back
directions = traceback_matrix(score_matrix, m+1, n+1, seq1, seq2)
directions = gsub(" ", "", directions)
translated = translate_directions(directions, seq1, seq2)
print(translated[[1]])
print(translated[[2]])
print(paste("Score:", score_matrix[m+1, n+1]))
}
# Recursively trace back the matrix and return a string of the directions to take
traceback_matrix = function(score_matrix, i, j, seq1, seq2, match, mismatch, gap) {
if (i==1 && j==1) {
return("")
}
current = score_matrix[i, j]
if (i == 1) {
# Only possible to come from the left
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), " L"))
} else if (j == 1) {
# Only possible to come from above
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), " D"))
}
# Possible directions: diagonal, left, up
diag = (substr(seq1, i-1, i-1) == substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] + 3) ||
(substr(seq1, i-1, i-1) != substr(seq2, j-1, j-1) && current == score_matrix[i-1, j-1] - 10)
left = score_matrix[i, j-1]-3 == current
up = score_matrix[i-1, j]-3 == current
if (diag) {
return(paste(traceback_matrix(score_matrix, i-1, j-1, seq1, seq2, match, mismatch, gap), "X"))
} else if (left) {
return(paste(traceback_matrix(score_matrix, i, j-1, seq1, seq2, match, mismatch, gap), "L"))
} else {
return(paste(traceback_matrix(score_matrix, i-1, j, seq1, seq2, match, mismatch, gap), "D"))
}
}
#translate directions to alignment
translate_directions = function(directions, seq1, seq2) {
seq1_aligned = ""
seq2_aligned = ""
i = 1
n = 1
directions = unlist(strsplit(directions, ""))
for (direction in directions) {
if (direction == "X") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
i = i + 1
n = n + 1
} else if (direction == "L") {
seq1_aligned = paste(seq1_aligned, "_", sep="")
seq2_aligned = paste(seq2_aligned, substr(seq2, n, n), sep="")
n = n + 1
} else if (direction == "D") {
seq1_aligned = paste(seq1_aligned, substr(seq1, i, i), sep="")
seq2_aligned = paste(seq2_aligned, "_", sep="")
i = i + 1
}
}
return(list(seq1_aligned, seq2_aligned))
}
global_align("AATCAGGT","CGTTTAGAC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-3)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-6)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-5)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-4)
global_align("TGCATC","CGTATC", match_score=3, mismatch_score=-10, gap_open=-3)
setwd("~/git/339n/class-notes")
renv::autoload()
renv::status()
renv::install('dslabs')
# ---- PRELIMINARIES ----
# Load necessary libraries and explore the BRCA dataset
#install.packages("dslabs")
library(dslabs)
str(brca)
?brca
# ---- VISUALIZATION ----
# Load additional libraries for visualization
install.packages("ggplot2")
install.packages("cowplot")
# ---- VISUALIZATION ----
# Load additional libraries for visualization
#install.packages("ggplot2")
#install.packages("cowplot")
library(ggplot2)
library(cowplot)
# ---- NAIVE BAYES CLASSIFICATION
# Establish a baseline using naive predictions
# (Several approaches: predictions, naive_predictions_1, naive_predictions_2)
# Evaluation of the baseline classifiers
install.packages("caret")
# ---- PRELIMINARIES ----
# Load necessary libraries and explore the BRCA dataset
#install.packages("dslabs")
library(dslabs)
str(brca)
?brca
# ---- DATA PREPARATION ----
# Define target variable and create a new dataframe
brca$y
set.seed(3) # Setting a seed for reproducibility
brca_df = data.frame(Target = brca$y, Feature = brca$x)
# Splitting data into training and testing sets
inTrain <- sample(1:nrow(brca_df), replace = FALSE, size = round(nrow(brca_df) * 0.9))
training <- brca_df[inTrain, ]
testing <- brca_df[-inTrain,]
str(training)
str(testing)
# ---- VISUALIZATION ----
# Load additional libraries for visualization
#install.packages("ggplot2")
#install.packages("cowplot")
library(ggplot2)
library(cowplot)
# Create visualizations for the training data
# Histograms and boxplots by feature and target class
p1 <- ggplot (training, aes(x= Feature.radius_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1) + theme_cowplot(12)
p2 <-ggplot (training, aes(x=Feature.perimeter_mean, fill= Target, color= Target)) + geom_histogram(binwidth=1)+theme_cowplot(12)
p3 <-ggplot (training, aes(x=Feature.symmetry_worst, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
p4 <-ggplot (training, aes(x=Feature.symmetry_se, fill= Target, color= Target)) + geom_boxplot() + theme_cowplot(12)
# Combine the plots into a grid
plot_grid(p1, p2, p3, p4, labels = c("A", "B", "C", "D"), label_size = 12)
malignant_radius = median (training$Feature.radius_mean[training$Target == "M"])
begnin_radius = median (training$Feature.radius_mean[training$Target == "B"])
predictions= c()
for (example in 1:nrow(training)){
if (training$Feature.radius_mean[example]> malignant_radius)
{
predictions[example]="M"
}else {
predictions[example]="B"
}
}
##Evaluation
predictions == training$Target
sum(predictions == training$Target)/nrow(training)
table(predictions)
table(training$Target)
#print the confusion matrix
table(predictions, training$Target)
naive_predictions_1 = rep("B", nrow(training))
sum(naive_predictions_1 == training$Target)/nrow(training)
table (naive_predictions_1, training$Target)
naive_predictions_2 = sample(c("M","B"), size=nrow(training), prob= c(196,316), replace=T)
sum(naive_predictions_2==training$Target)/nrow(training)
table(naive_predictions_2, training$Target)
# ---- NAIVE BAYES CLASSIFICATION
# Establish a baseline using naive predictions
# (Several approaches: predictions, naive_predictions_1, naive_predictions_2)
# Evaluation of the baseline classifiers
#install.packages("caret")
library (caret)
install.packages("e1071")
#install.packages("e1071")
library(e1071)  # This library contains the naiveBayes function
nb_model = naiveBayes(Target ~ ., data = training)
nb_predict = predict(nb_model, testing)
nb_accuracy = sum(nb_predict == testing$Target) / nrow(testing)  # Calculate accuracy
nb_predict  # The predictions from the Naive Bayes model
#kNN
library(class)
train_features <- training[ , -1]  # Exclude the target column
test_features <- testing[ , -1]
train_target <- training$Target
# Use kNN to predict on the test set (k = 3 as an example)
knn_predictions <- knn(train = train_features, test = test_features, cl = train_target, k = 3)
knn_accuracy <- sum(knn_predictions == testing$Target) / length(testing$Target)
print(knn_accuracy)
table(knn_predictions, testing$Target)
# Try different values of k
k_values <- c(1, 3, 5, 7, 10)
accuracies <- c()
for (k in k_values) {
knn_predictions <- knn(train = train_features, test = test_features, cl = train_target, k = k)
knn_accuracy <- sum(knn_predictions == testing$Target) / length(testing$Target)
accuracies <- c(accuracies, knn_accuracy)
print(paste("k =", k, "Accuracy =", knn_accuracy))
}
# Plot the accuracy against k values
plot(k_values, accuracies, type = "b", xlab = "k Value", ylab = "Accuracy", main = "Accuracy vs k")
install.packages("caret")
#install.packages("caret")
library(caret)
# Define training control with kNN
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Train the kNN model with a different distance metric (e.g., Manhattan)
set.seed(123)
knn_model <- train(Target ~ ., data = training,
method = "knn",
trControl = train_control,
tuneGrid = expand.grid(k = c(1, 3, 5, 7, 10)),
preProcess = c("center", "scale"),
metric = "Accuracy",
tuneLength = 10)
print(knn_model)
# Use caret package for cross-validation
set.seed(123)
knn_cv_model <- train(Target ~ ., data = training,
method = "knn",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = expand.grid(k = 3:10))
print(knn_cv_model)
plot(knn_cv_model)
# Using the kNN Package for Weighted Voting
# Train a kNN model using distance weighting
install.packages("kknn")
gc()
# Using the kNN Package for Weighted Voting
# Train a kNN model using distance weighting
#install.packages("kknn")
library(kknn)
kknn_model <- train.kknn(Target ~ ., data = training,
kmax = 10, distance = 2, kernel = "optimal")
kknn_predictions <- predict(kknn_model, newdata = testing)
kknn_accuracy <- sum(kknn_predictions == testing$Target) / nrow(testing)
kknn_accuracy
library(factoextra)
# k-means clustering
install.packages("factoextra")
install.packages("cluster")
library(factoextra)
library(cluster)
# Data scaling, determining optimal number of clusters, and k-means clustering
# Visualization of clustering results
brca_df = data.frame(Target=brca$y, Feature= brca$x)
str(brca_df)
brca_df <- na.omit(brca_df)
brca_scaled= scale(brca_df[,-1])
fviz_nbclust(brca_scaled, kmeans, method = "wss")
# Elbow method looks at the percentage of variance explained as a function
# of the number of clusters: one should choose a number of clusters so that
#adding another cluster doesn’t give much better modeling of the data.
#cluster package
gap_stat <- clusGap(brca_scaled,
FUN = kmeans,
nstart = 25,
K.max = 10,
B = 50)
fviz_gap_stat(gap_stat)
# Let's perform k-means clustering using k of 2
set.seed(1)
#perform k-means clustering on the scaled dataset
km <- kmeans(brca_scaled, centers = 2, nstart = 25)
km
#Let's visualize the clusters formed by k-means using the fviz_cluster function.
fviz_cluster(km, data = brca_scaled)
brca_df[430,1]
brca_df[300,1]
str(km)
#Let's calculate the mean of each variable for each cluster.
#use the aggregate function to apply the mean function to the brca_scaled dataset,
#which is grouped by the cluster assignment from the k-means result.
aggregate(brca_scaled, by=list(cluster=km$cluster), mean)
km$cluster
table <- table(km$cluster, brca_df$Target)
table
#Determine the most frequent label in each cluster.
cluster_to_label_map <- apply(table, 1, function(row) {
if (row['B'] > row['M']) {
return('B')
} else {
return('M')
}
})
# Use the mapping to convert cluster numbers to predicted labels.
predicted_labels <- factor(sapply(km$cluster, function(cluster_number) {
cluster_to_label_map[as.character(cluster_number)]
}), levels = levels(brca_df$Target))
# cross-tabulation of cluster assignments and true labels
table <- table(km$cluster, brca_df$Target)
print(table)
# Calculate "accuracy"
accuracy <- sum(predicted_labels == brca_df$Target) / length(brca_df$Target)
print(accuracy)
renv::status()
renv::snapshot()
# Train the kNN model with a different distance metric (e.g., Manhattan)
set.seed(123)
knn_model <- train(Target ~ ., data = training,
method = "knn",
trControl = train_control,
tuneGrid = expand.grid(k = c(1, 3, 5, 7, 10)),
preProcess = c("center", "scale"),
metric = "Accuracy",
tuneLength = 10)
print(knn_model)
# Use caret package for cross-validation
set.seed(123)
knn_cv_model <- train(Target ~ ., data = training,
method = "knn",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = expand.grid(k = 3:10))
print(knn_cv_model)
plot(knn_cv_model)
# Data scaling, determining optimal number of clusters, and k-means clustering
# Visualization of clustering results
brca_df = data.frame(Target=brca$y, Feature= brca$x)
str(brca_df)
brca_df <- na.omit(brca_df)
brca_scaled= scale(brca_df[,-1])
fviz_nbclust(brca_scaled, kmeans, method = "wss")
# Elbow method looks at the percentage of variance explained as a function
# of the number of clusters: one should choose a number of clusters so that
#adding another cluster doesn’t give much better modeling of the data.
#cluster package
gap_stat <- clusGap(brca_scaled,
FUN = kmeans,
nstart = 25,
K.max = 10,
B = 50)
fviz_gap_stat(gap_stat)
# Let's perform k-means clustering using k of 2
set.seed(1)
#perform k-means clustering on the scaled dataset
km <- kmeans(brca_scaled, centers = 2, nstart = 25)
km
#Let's visualize the clusters formed by k-means using the fviz_cluster function.
fviz_cluster(km, data = brca_scaled)
brca_df[430,1]
brca_df[300,1]
str(km)
#Let's calculate the mean of each variable for each cluster.
#use the aggregate function to apply the mean function to the brca_scaled dataset,
#which is grouped by the cluster assignment from the k-means result.
aggregate(brca_scaled, by=list(cluster=km$cluster), mean)
km$cluster
table <- table(km$cluster, brca_df$Target)
table
#Determine the most frequent label in each cluster.
cluster_to_label_map <- apply(table, 1, function(row) {
if (row['B'] > row['M']) {
return('B')
} else {
return('M')
}
})
# Use the mapping to convert cluster numbers to predicted labels.
predicted_labels <- factor(sapply(km$cluster, function(cluster_number) {
cluster_to_label_map[as.character(cluster_number)]
}), levels = levels(brca_df$Target))
# cross-tabulation of cluster assignments and true labels
table <- table(km$cluster, brca_df$Target)
print(table)
# Calculate "accuracy"
accuracy <- sum(predicted_labels == brca_df$Target) / length(brca_df$Target)
print(accuracy)
